%% Brainbeats_analyze
% 
% Run statistics on EEG and HRV features generated by BrinBeats_process
%
% Copyright (C) - Cedric Cannard, 2023


function brainbeats_analyze()

clear; close all; clc
eeglab; close;
% mainDir = fileparts(which('eegplugin_BrainBeats.m')); cd(mainDir);
mainDir = 'C:\Users\Tracy\Desktop\trance_mindwandering'; cd(mainDir);
outDir = fullfile(mainDir,'statistics');  
mkdir(outDir); cd(mainDir)

normalize = false;

HRV = table.empty;
EEG = table.empty;

% Condition/group 1
% [files,path] = uigetfile('*.mat','Select feature files for condition/group 1','MultiSelect','on');
files = {'sub-01_mindwandering_session-1_features.mat'	'sub-02_mindwandering_session-2_features.mat'	'sub-03_mindwandering_session-2_features.mat'	'sub-04_mindwandering_session-1_features.mat'	'sub-05_mindwandering_session-1_features.mat'	'sub-06_mindwandering_session-1_features.mat'	'sub-07_mindwandering_session-2_features.mat'	'sub-08_mindwandering_session-1_features.mat'	'sub-09_mindwandering_session-1_features.mat'	'sub-10_mindwandering_session-1_features.mat'	'sub-11_mindwandering_session-1_features.mat'	'sub-12_mindwandering_session-1_features.mat'	'sub-13_mindwandering_session-2_features.mat'};
path = fullfile(mainDir,'data');
for iFile = 1:length(files)
    
    % Load features generated by brainbeats_process
    load(fullfile(path,files{iFile}),'Features');

    % Extract relevant features in Table format: condition/group 1
    [hrv, eeg] = extract_features(Features);   
    hrv(:,size(hrv,2)+1) = table(1);    % label 1 for condition/group 1
    hrv.Properties.VariableNames(end) = {'Label'};

    % Merge into Master tables
    try
        HRV(iFile,:) = hrv;
        EEG(iFile,:) = eeg;
    catch
        error('Failed to import HRV/EEG features for file %s into Master table. \nThis can occur if you computed different features for this files', files{iFile})
    end
end

% Condition/group 2
% [files,path] = uigetfile('*.mat','Select feature files for condition/group 2','MultiSelect','on');
files = {'sub-01_trance_session-1_features.mat'	'sub-02_trance_session-1_features.mat'	'sub-03_trance_session-1_features.mat'	'sub-04_trance_session-1_features.mat'	'sub-05_trance_session-1_features.mat'	'sub-06_trance_session-1_features.mat'	'sub-07_trance_session-2_features.mat'	'sub-08_trance_session-1_features.mat'	'sub-09_trance_session-1_features.mat'	'sub-10_trance_session-1_features.mat'	'sub-11_trance_session-1_features.mat'	'sub-12_trance_session-1_features.mat'	'sub-13_trance_session-2_features.mat'};
for iFile = 1:length(files)

    % Extract relevant features in Table format: condition/group 1
    load(fullfile(path,files{iFile}),'Features');
    [hrv, eeg] = extract_features(Features);   
    hrv(:,size(hrv,2)+1) = table(2);    % label 1 for condition/group 1
    hrv.Properties.VariableNames(end) = {'Label'};

    % Merge into Master tables
    try
        HRV(end+1,:) = hrv;
        EEG(end+1,:) = eeg;
    catch
        error('Failed to import HRV/EEG features for file %s into Master table. \nThis can occur if you computed different features for this files', files{iFile})
    end
end

% Normalize 
if normalize
    disp('Normalizing data across subjects')
    HRV_norm = array2table(zscore(table2array(HRV(:,1:end-1))));
    HRV_norm(:,end+1) = HRV(:,end);
    HRV_norm.Properties.VariableNames = HRV.Properties.VariableNames;
    HRV = HRV_norm;
end

fprintf('Done extracting and reformatting features for analysis. \n')
writetable(HRV,fullfile(outDir,'Features_HRV.csv'))
writetable(HRV,fullfile(outDir,'Features_EEG.csv'))

%% Feature selection using random forest (train bagged ensemble of 5000 
% regression trees).

% clear; close all; clc
% Load tables if erased for some reason
% HRV = readtable(fullfile(outDir,'Features_HRV.csv'),'VariableNamingRule','preserve');
% EEG = readtable(fullfile(outDir,'Features_EEG.csv'),'VariableNamingRule','preserve');

% % Quick check of number of files per condition/group
labels = categorical(HRV.Label);
summary(labels)

% use interaction-curvature test to select split predictors ('allsplits',
%       'curvature', 'interaction-curvature')
% use surrogate splits to increase accuracy (when dataset includes missing values)
t = templateTree('NumVariablesToSample','all','PredictorSelection','interaction-curvature','Surrogate','on');
Mdl = fitrensemble(HRV(:,1:end-1),HRV.Label,'Method','Bag', ...
    'NumLearningCycles',5000,'Learners',t);
yHat = oobPredict(Mdl);
R2 = corr(Mdl.Y,yHat)^2;  
fprintf('Model R2 = %g (i.e., variability explained by the model) \n', round(R2,2)) % variability around the mean explained by mdl

% Permute out-of-bag observations among trees to estimate features' importance
impOOB = oobPermutedPredictorImportance(Mdl);
figure('color','w'); 
subplot(2,1,1)
bar(impOOB)
% title('Unbiased Predictor Importance Estimates'); 
xlabel('Predictor variable'); ylabel('Importance'); 
h = gca; h.XTickLabel = Mdl.PredictorNames; h.XTickLabelRotation = 45;
h.TickLabelInterpreter = 'none';

% Compare predictor importance estimates by permuting out-of-bag observations 
% and those estimates obtained by summing gains in the mean squared error
% (MSE) due to splits on each predictor. Also, obtain predictor association 
% measures estimated by surrogate splits. 
[impGain,predAssociation] = predictorImportance(Mdl);
hold on; 
plot(1:numel(Mdl.PredictorNames),impOOB,'linewidth',3)
plot(1:numel(Mdl.PredictorNames),impGain,'linewidth',3)
title('Predictor Importance Estimation Comparison')
legend('', 'OOB permuted','MSE improvement')
grid on

% Assess predictive measure association to indicate similarity between
% decision rules that split observations. The best surrogate decision split 
% yields the maximum predictive measure of association. You can infer the 
% strength of the relationship between pairs of predictors using the elements 
% of predAssociation. Larger values indicate more highly correlated pairs of 
% predictors.
subplot(2,1,2)
imagesc(predAssociation); title('Predictor Association Estimates')
colorbar; h = gca; h.XTickLabel = Mdl.PredictorNames;
h.XTickLabelRotation = 45; h.TickLabelInterpreter = 'none';
h.YTickLabel = Mdl.PredictorNames;

% Save figure
set(findall(gcf,'type','axes'),'fontSize',10,'fontweight','bold'); 
saveas(gcf,fullfile(path,'diagnostic_HRV-features.png')); close(gcf);

% Input the strongest association here (yellow or green), if < .7 association
% is not high enough to indicate strong relationship between the 2 predictors
% predAssociation(1,2)  % row,column
% predAssociation(1,7)  % row,column
% predAssociation(1,8)  % row,column

% Run Random Forest again using selected predictors and compare R2
t = templateTree('PredictorSelection','interaction-curvature','Surrogate','off'); % For reproducibility of random predictor selections
MdlReduced  = fitrensemble(HRV(:,{ 'NN_mean' 'NN_skew' 'NN_kurt' 'RMSSD' 'pNN50' 'SD1/SD2'}), ...
    HRV.Label,'Method','Bag','NumLearningCycles',5000,'Learners',t);
yHatReduced = oobPredict(MdlReduced);
r2Reduced = corr(Mdl.Y,yHatReduced)^2;
fprintf('Model R2 = %g \n', round(R2,2))
fprintf('Reduced model R2 = %g \n', round(r2Reduced,2))

% Keep only best predictors and train classifiers models
HRV = HRV(:,MdlReduced.PredictorNames); 
writetable(HRV,fullfile(outDir,'Features_selected_HRV.csv'))

%% CORRELATION PLOT

% Condition 1
C1 = corrcoef([table2array(HRV) table2array(EEG) ]);
labels = [HRV.Properties.VariableNames EEG.Properties.VariableNames ];
plot_corrmatrix(C1,labels)


load hospital
X = [hospital.Weight hospital.BloodPressure];
figure('color','w'); 
subplot(2,1,1); plotmatrix(C); title('Scatter plot'); 
R = corrcoef(X);
subplot(2,1,2); imagesc(R); title('Correlation coefficient (R)'); colorbar


%% PCA-dimension reduction 

X = table2array(HRV);
% X = zscore(X); % normalize

% Before thinking about dimension reduction, the first step is to redefine 
% a coordinate system (x',y'), such that x' is along the first principal 
% component, and y' along the second component (and so on, if there are more 
% variables). The new variables are "Score" variable
% As in the original data, each row is an observation, and each column is a dimension.
% These data are just like your original data, except it is as if you 
% measured them in a different coordinate system -- the principal axes.

[coeff,score,latent,~,explained] = pca(X); % uses Singular Value Decomposition (default)

% % Calculate eigenvalues and eigenvectors of the covariance matrix
% covarianceMatrix = cov(X);
% [V,D] = eig(covarianceMatrix);
% coeff
% V

% score % projections of the original data on the principal component vector space.
%  (same as doing X*coeff)

% The columns of score are orthogonal to each other.
% corrcoef(score)

% The variances of these vectors are the eigenvalues of the covariance matrix,
% and are also the output "latent".
% var(score)'
% latent

% Now you can think about dimension reduction. Take a look at the variable 
% 'explained'. It tells you how much of the variation is captured by each column 
% of 'score'. Here is where you have to make a judgement call. How much 
% of the total variation are you willing to ignore? 
% One guideline is that if you plot explained, there will often be an "elbow" 
% in the plot, where each additional variable explains very little additional 
% variation. Keep only the components that add a lot more explanatory power, 
% and ignore the rest.
% If first 3 components together explain 87% of the variation; suppose 
% that's good enough. Then, you would only keep those 3 dimensions (the first 3 
% columns of score). You will have 7 observations in 3 dimensions (variables) instead of 5.

% explained' % variation catpured by each column in 'score'

figure('color','w'); 
plot(explained,'.-','linewidth',2); title('Explained variance by each component')
% bar(sqrt(sum(score.^2, 1)))
sum(explained(1:2))

% make a biplot (PC1 on x-axis and PC2 on y-axis)
xPC = 1;
yPC = 2;
figure; hold on
for nc = 1:N
    plot([0 coeff(nc,xPC)],[0 coeff(nc,yPC)],'b'); 
end


% the eigenvectors (coeff) gives the weight of the original predictors for 
% each PC. 
% coeff(i,j).^2 % gives the % weighting of the i'th original predictor to the j'th PC.

% You want to explain 95% of the variance: find where cumsum(explained) >= 95. 
% Let's suppose this requires 4 of the PCs. you study the relationship of 
% those 4 PCs to the original variables, by inspecting the first 4 columns
% of coeff. If just a few of your original variables contribute to the first
% 4 PCs, this is great news! You drop all the other variables -- losing a 
% little bit of the explained variance -- and you are all set.
% But suppose your first column of coeff looks like this:
% coeff = [0.37; 0.42; 0.62; 0.41; ...];
% where the first PC has significant contribution from every variable? 
% In that case, you cannot isolate a small number of the original predictors,
% and you cannot do what you want. Then it is sad trombone for you.


% Scatter plot of the first two components (normalize variables original data by dividng by SD)
% figure; hold on
% h = plot(score(:,1),score(:,2),'ro');
% set(h,'MarkerSize',16)
% set(gca,'XLim',[-1 1],'YLim',[-1 1],'Box','on')
% axis square
% xlabel('Component 1')
% ylabel('Component 2')
% % Add a circle
% p = nsidedpoly(1000, 'Center', [0 0], 'Radius', 0.8);
% plot(p, 'FaceColor', 'w', 'EdgeColor', 'r')

% Recover the original data from the PC
% score * coeff'

% "In general, every variable contributes to every principal component." 
% In my example with 5 variables, if they had all been very highly 
% correlated with each other, that all 5 of them contributed significantly 
% to the first principal component. You could not eliminate any of the 
% original variables without significant loss of information.
% you can't eliminate either the x-axis variable or the y-axis variable. 
% Instead, you choose a linear combination of them that captures the maximal 
% variation. there are techniques like varimax, applied after PCA, that
% allow you to remove some of the original variables.

% Default (normalized varimax) rotation: first 3 principal components.
% [1] Harman, H. H. Modern Factor Analysis. 3rd ed. Chicago: University of Chicago Press, 1976.
% [2] Lawley, D. N., and A. E. Maxwell. Factor Analysis as a Statistical Method. 2nd ed. New York: American Elsevier Publishing, 1971. 
% LPC = pca(X);
[L1,T] = rotatefactors(coeff(:,1:2));
inv(T'*T) % Correlation matrix of the rotated factors
